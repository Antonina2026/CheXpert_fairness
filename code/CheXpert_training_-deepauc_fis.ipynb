{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14064993,"sourceType":"datasetVersion","datasetId":8952502},{"sourceId":14382753,"sourceType":"datasetVersion","datasetId":9185341}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-deps /kaggle/input/libauc-1-2-0/libauc-1.2.0-py3-none-any.whl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:30.183149Z","iopub.execute_input":"2026-02-08T10:36:30.183740Z","iopub.status.idle":"2026-02-08T10:36:31.644992Z","shell.execute_reply.started":"2026-02-08T10:36:30.183714Z","shell.execute_reply":"2026-02-08T10:36:31.644124Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/libauc-1-2-0/libauc-1.2.0-py3-none-any.whl\nlibauc is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from libauc.losses import AUCM_MultiLabel, CrossEntropyLoss\nfrom libauc.optimizers import PESG, Adam\nfrom libauc.models import densenet121 as DenseNet121\nfrom libauc.datasets import CheXpert\nfrom libauc.metrics import auc_roc_score # for multi-task\n\nfrom PIL import Image\nimport numpy as np\nimport torch \nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F   \n\nimport pandas as pd\nimport cv2\n\nfrom torchvision.models import densenet121, DenseNet121_Weights\nimport torchvision.transforms as tfs\n\nimport torch.nn as nn\n\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:31.646681Z","iopub.execute_input":"2026-02-08T10:36:31.646929Z","iopub.status.idle":"2026-02-08T10:36:35.076913Z","shell.execute_reply.started":"2026-02-08T10:36:31.646906Z","shell.execute_reply":"2026-02-08T10:36:35.076041Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"fair_scaling_coef = 0.5\nfair_scaling_sinkhorn_blur = .1\nfair_scaling_temperature = 1.\nfair_right_asymptote = 1.\n\nlabels = [\n    'Enlarged Cardiomediastinum',\n    'Cardiomegaly',\n    'Lung Opacity',\n    'Lung Lesion',\n    'Edema',\n    'Consolidation',\n    'Pneumonia',\n    'Atelectasis',\n    'Pneumothorax',\n    'Pleural Effusion',\n    'Pleural Other',\n    'Fracture',\n    'Support Devices',\n    'No Finding'\n]\n\nlabels_small = [\n    'Cardiomegaly',\n    'Pneumonia',\n    'Pleural Effusion',\n   'Fracture',\n    'No Finding'\n]\n\nlabels_abbr_small = [\n    'Cd',\n    'Pa',\n    'Ef',\n    'Fr',\n    'NF'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:35.077941Z","iopub.execute_input":"2026-02-08T10:36:35.078389Z","iopub.status.idle":"2026-02-08T10:36:35.083389Z","shell.execute_reply.started":"2026-02-08T10:36:35.078364Z","shell.execute_reply":"2026-02-08T10:36:35.082587Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def df_age_disaggregation(df, left_edge = 15, right_edge = 105, bin_width = 5):\n\n    # define age bins and labels\n    bins = list(range(left_edge, right_edge, bin_width)) \n    labels = [f\"{b}–{b+4}\" for b in bins[:-1]]\n\n    if right_edge>105:\n        bins.append(120)\n        labels.append('>=100')\n    \n    df[\"age_group\"]= pd.cut(\n        df[\"Age\"],\n        bins=bins,\n        labels=labels,\n        right=False)\n    \n    return df\n\ndef age_group_to_index(age_group):\n  \n    age_groups_order = [\n        '15–19', '20–24', '25–29', '30–34', '35–39',\n        '40–44', '45–49', '50–54', '55–59', '60–64',\n        '65–69', '70–74', '75–79', '80–84', '85–89',\n        '90–94', '95–99'\n    ]\n\n    # Build lookup table\n    group_to_idx = {g: i for i, g in enumerate(age_groups_order)}\n\n    # Convert input to python list if needed\n    if isinstance(age_group, torch.Tensor):\n        age_group = age_group.tolist()\n\n    indices = []\n    for g in age_group:\n        if g not in group_to_idx:\n            raise ValueError(f\"Unknown age group: {g}\")\n        indices.append(group_to_idx[g])\n\n    return torch.tensor(indices, dtype=torch.long)\n\n# Compute group performance (offline / per epoch)\n\ndef compute_group_auc(y_true, y_prob, sensitive_attr):\n    \"\"\"\n    y_true: (N, C)\n    y_prob: (N, C)\n    sensitive_attr: (N,)\n    \"\"\"\n    group_aucs = {}\n    groups = np.unique(sensitive_attr)\n\n    for g in groups:\n        mask = sensitive_attr == g\n        if mask.sum() < 10:\n            continue  # avoid unstable estimates\n        group_aucs[g] = np.mean(auc_roc_score(\n            y_true[mask], y_prob[mask]\n        ))\n    return group_aucs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:35.084212Z","iopub.execute_input":"2026-02-08T10:36:35.084468Z","iopub.status.idle":"2026-02-08T10:36:35.098433Z","shell.execute_reply.started":"2026-02-08T10:36:35.084446Z","shell.execute_reply":"2026-02-08T10:36:35.097767Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"pip install geomloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:35.100092Z","iopub.execute_input":"2026-02-08T10:36:35.100300Z","iopub.status.idle":"2026-02-08T10:36:38.276692Z","shell.execute_reply.started":"2026-02-08T10:36:35.100284Z","shell.execute_reply":"2026-02-08T10:36:38.275756Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: geomloss in /usr/local/lib/python3.11/dist-packages (0.2.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from geomloss) (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from geomloss) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->geomloss) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->geomloss) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->geomloss) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->geomloss) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->geomloss) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->geomloss) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->geomloss) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->geomloss) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->geomloss) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->geomloss) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->geomloss) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->geomloss) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->geomloss) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->geomloss) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->geomloss) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# FIS code\n'''\n    Reference: \n    \n    @misc{Luo2024,\n      title={FairVision: Equitable Deep Learning for Eye Disease Screening via Fair Identity Scaling}, \n      author={Yan Luo and Muhammad Osama Khan and Yu Tian and Min Shi and Zehao Dou and Tobias Elze and Yi Fang and Mengyu Wang},\n      year={2024},\n      eprint={2310.02492},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2310.02492}\n    }\n'''\n\n# GeomLoss is a third-party research library. Purpose: Efficient Optimal Transport (OT) loss computations for ML\n# Repository: https://github.com/jeanfeydy/geomloss\nfrom geomloss import SamplesLoss\n\nclass Fair_Loss_Scaler(nn.Module):\n    def __init__(self, level='individual', fair_scaling_group_weights=None, \n                    fair_scaling_temperature=1., fair_scaling_coef=.5, sinkhorn_blur=0.1, right_asymptote=2.):\n        super().__init__()\n        self.level = level\n        self.fair_scaling_temperature = fair_scaling_temperature\n        self.sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=sinkhorn_blur)\n        self.fair_scaling_coef = fair_scaling_coef\n        #self.fair_scaling_coef = nn.Parameter(torch.tensor(fair_scaling_coef))\n        self.right_asymptote = right_asymptote\n        \n    def forward(self, x, smp_xs, attr):\n                \n        # x: (B, C) → reduce to (B,)\n        if x.ndim == 2:\n            x = x.mean(dim=1)\n\n        # individual weights\n        individual_weights = torch.softmax(x.detach() / self.fair_scaling_temperature, dim=0) * self.right_asymptote\n    \n        # total distribution\n        # ttl_smp_x = torch.cat(smp_xs)\n        ttl_smp_x = torch.stack(smp_xs)  # [num_groups]\n    \n        # distances_distributions = torch.zeros(len(smp_xs), device=x.device)\n        distances_distributions = torch.zeros(len(smp_xs), device=x.device, dtype=x.dtype)\n    \n        # for i, smp_x in enumerate(smp_xs):\n        #     distances_distributions[i] = self.sinkhorn_loss(\n        #         smp_x.view(1, -1, 1),\n        #         ttl_smp_x.view(1, -1, 1)\n        #     )\n\n        for i, smp_x in enumerate(smp_xs):\n            distances_distributions[i] = self.sinkhorn_loss(\n                smp_x.view(1, 1, 1),           # (B=1, N=1, D=1)\n                ttl_smp_x.view(1, -1, 1)       # (B=1, M=G, D=1)\n            )\n    \n        group_weights = torch.softmax(\n            distances_distributions[attr.long()] / self.fair_scaling_temperature,\n            dim=0\n        ) * self.right_asymptote\n    \n        loss = (\n            ((1 - self.fair_scaling_coef) * individual_weights\n             + self.fair_scaling_coef * group_weights)\n            * x\n        ).mean()\n    \n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:38.277968Z","iopub.execute_input":"2026-02-08T10:36:38.278301Z","iopub.status.idle":"2026-02-08T10:36:38.291102Z","shell.execute_reply.started":"2026-02-08T10:36:38.278265Z","shell.execute_reply":"2026-02-08T10:36:38.290320Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class CheXpert(Dataset):\n    '''\n    Reference: \n        @inproceedings{yuan2021robust,\n            title={Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification},\n            author={Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},\n            booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n            year={2021}\n            }\n    '''\n    def __init__(self, \n                 csv_path, \n                 image_root_path='',\n                 image_size=320,\n                 class_index=0, \n                 seed=123,\n                 # verbose=True,\n                 train_cols=labels_small,\n                 mode='train'):\n        \n    \n        # load data from csv\n        self.df = pd.read_csv(csv_path)\n        self.df['Path'] = self.df['Path'].str.replace(\"\\\\\", \"/\")\n        self.df['Path'] = self.df['Path'].str.replace('CheXpert-v1.0-small/', '')\n        self.df['Path'] = self.df['Path'].str.replace('CheXpert-v1.0/', '')\n       \n        self._num_images = len(self.df)\n        \n        assert class_index in [-1, 0, 1, 2, 3, 4], 'Out of selection!'\n        assert image_root_path != '', 'You need to pass the correct location for the dataset!'\n\n        if class_index == -1: # 5 classes\n            print ('Multi-label mode: True, Number of classes: [%d]'%len(train_cols))\n            self.select_cols = train_cols\n            self.value_counts_dict = {}\n            for class_key, select_col in enumerate(train_cols):\n                class_value_counts_dict = self.df[select_col].value_counts().to_dict()\n                self.value_counts_dict[class_key] = class_value_counts_dict\n        else:       # 1 class\n            self.select_cols = [train_cols[class_index]]  # this var determines the number of classes\n            self.value_counts_dict = self.df[self.select_cols[0]].value_counts().to_dict()\n        \n        self.mode = mode\n        self.class_index = class_index\n        self.image_size = image_size\n        \n        self._images_list =  [image_root_path+path for path in self.df['Path'].tolist()]\n        if class_index != -1:\n            self._labels_list = self.df[train_cols].values[:, class_index].tolist()\n        else:\n            self._labels_list = self.df[train_cols].values.tolist()\n\n        sex_map = {\"Male\": 1, \"Female\": 0}\n        self.sex = self.df[\"Sex\"].map(sex_map).values\n\n        self.sex_str = self.df[\"Sex\"].values\n\n        self.age = self.df[\"Age\"].values\n\n        data = self.df[\"Age\"].values\n        df = df_age_disaggregation(pd.DataFrame(data, columns=['Age']))\n        self.age_group = df['age_group']\n\n        \n        self.age_bin = age_group_to_index(self.age_group)\n        assert self.age_bin.min() >= 0 and self.age_bin.max() <= 16\n \n    @property        \n    def class_counts(self):\n        return self.value_counts_dict\n    \n    @property\n    def num_classes(self):\n        return len(self.select_cols)\n       \n    @property  \n    def data_size(self):\n        return self._num_images \n    \n    def image_augmentation(self, image):\n        img_aug = tfs.Compose([tfs.RandomAffine(degrees=(-15, 15), translate=(0.05, 0.05), scale=(0.95, 1.05), fill=128)]) # pytorch 3.7: fillcolor --> fill\n        image = img_aug(image)\n        return image\n    \n    def __len__(self):\n        return self._num_images\n    \n    def __getitem__(self, idx):\n\n        image = cv2.imread(self._images_list[idx], 0)\n        image = Image.fromarray(image)\n        if self.mode == 'train':\n            image = self.image_augmentation(image)\n        image = np.array(image)\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n        \n        # resize and normalize; e.g., ToTensor()\n        image = cv2.resize(image, dsize=(self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)  \n        image = image/255.0\n        __mean__ = np.array([[[0.485, 0.456, 0.406]]])\n        __std__ =  np.array([[[0.229, 0.224, 0.225]  ]]) \n        image = (image-__mean__)/__std__\n        image = image.transpose((2, 0, 1)).astype(np.float32)\n        if self.class_index != -1: # multi-class mode\n            label = np.array(self._labels_list[idx]).reshape(-1).astype(np.float32)\n        else:\n            label = np.array(self._labels_list[idx]).reshape(-1).astype(np.float32)\n\n        attributes = {\n            \"Sex\": self.sex[idx],  \n            \"Age\": self.age[idx],\n            \"AgeBin\": self.age_bin[idx]\n        }\n                \n        return image, label, attributes\n\n\n\nif __name__ == '__main__':\n\n    root = '/kaggle/input/chexpert-v1-0/CheXpert-v1.0/'\n    # root = \"D:/chexpertchestxrays/CheXpert-v1.0/CheXpert-v1.0/\"\n    traindSet = CheXpert(csv_path=root+'train.csv', image_root_path=root, image_size=320, mode='train', class_index=0)\n    testSet =  CheXpert(csv_path=root+'valid.csv',  image_root_path=root, image_size=320, mode='valid', class_index=0)\n\n    trainloader =  torch.utils.data.DataLoader(traindSet, batch_size=32, num_workers=2, drop_last=True, shuffle=True)\n    testloader =  torch.utils.data.DataLoader(testSet, batch_size=32, num_workers=2, drop_last=False, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:38.291858Z","iopub.execute_input":"2026-02-08T10:36:38.292041Z","iopub.status.idle":"2026-02-08T10:36:39.058582Z","shell.execute_reply.started":"2026-02-08T10:36:38.292027Z","shell.execute_reply":"2026-02-08T10:36:39.057734Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def set_all_seeds(SEED):\n    # REPRODUCIBILITY\n    torch.manual_seed(SEED)\n    np.random.seed(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nroot = '/kaggle/input/chexpert-v1-0/CheXpert-v1.0/'\ntraindSet = CheXpert(csv_path=root+'train.csv', image_root_path=root, image_size=224, mode='train', class_index=-1)\ntestSet =  CheXpert(csv_path=root+'valid.csv',  image_root_path=root, image_size=224, mode='valid', class_index=-1)\ntrainloader =  torch.utils.data.DataLoader(traindSet, batch_size=32, num_workers=0, shuffle=True, pin_memory=False)\ntestloader =  torch.utils.data.DataLoader(testSet, batch_size=32, num_workers=0, shuffle=False, pin_memory=False)\n\n# paramaters\nSEED = 123\nBATCH_SIZE = 32\nlr = 0.1 \nepoch_decay = 2e-3\n# epoch_decay = 0.95 \nweight_decay = 1e-5\nmargin = 1.0\ntotal_epochs = 2\n\n# model\nset_all_seeds(SEED)\n# model = DenseNet121(pretrained=True, last_activation=None, activations='relu', num_classes=5)\nmodel = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\nmodel.classifier = nn.Linear(model.classifier.in_features, 5) \n# model = model.cuda()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# define loss & optimizer\nloss_fn = AUCM_MultiLabel(num_classes=5)\noptimizer = PESG(model, \n                 loss_fn=loss_fn,\n                 lr=lr, \n                 margin=margin, \n                 epoch_decay=epoch_decay, \n                 weight_decay=weight_decay)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:39.060256Z","iopub.execute_input":"2026-02-08T10:36:39.060512Z","iopub.status.idle":"2026-02-08T10:36:40.667153Z","shell.execute_reply.started":"2026-02-08T10:36:39.060491Z","shell.execute_reply":"2026-02-08T10:36:40.666590Z"}},"outputs":[{"name":"stdout","text":"Multi-label mode: True, Number of classes: [5]\nMulti-label mode: True, Number of classes: [5]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# training densenet121 without fairness consideration\nprint ('Start Training')\nprint ('-'*30)\n\nbest_val_auc = 0 \n# Measure training time\nstart = time.time()\nfor epoch in range(total_epochs):\n    if epoch > 0:\n        optimizer.update_regularizer(decay_factor=10)    \n\n    for idx, data in enumerate(trainloader):\n      # train_data, train_labels = data\n      train_data, train_labels, train_attributes = data  \n      train_data, train_labels  = train_data.cuda(), train_labels.cuda()\n      y_pred = model(train_data)\n      y_pred = torch.sigmoid(y_pred)\n      loss = loss_fn(y_pred, train_labels)\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n              \n      # validation  \n      if idx % 400 == 0:\n         model.eval()\n         with torch.no_grad():    \n              test_pred = []\n              test_true = [] \n              test_attributes_true = [] \n              for jdx, data in enumerate(testloader):\n                  # test_data, test_labels = data\n                  test_data, test_labels, test_attributes = data\n                  test_data = test_data.cuda()\n                  y_pred = model(test_data)\n                  y_pred = torch.sigmoid(y_pred)\n                  test_pred.append(y_pred.cpu().detach().numpy())\n                  test_true.append(test_labels.numpy())\n                  test_attributes_true.append(test_attributes[\"Sex\"].cpu().numpy())\n            \n              test_true = np.concatenate(test_true)\n              test_pred = np.concatenate(test_pred)\n              test_attributes_true = np.concatenate(test_attributes_true)\n\n              group_aucs = compute_group_auc(test_true, test_pred, test_attributes_true)\n          \n              val_auc_mean = np.mean(auc_roc_score(test_true, test_pred)) \n              model.train()\n\n              if best_val_auc < val_auc_mean:\n                 best_val_auc = val_auc_mean\n                 torch.save(model.state_dict(), 'aucm_pretrained_model.pth')\n\n              print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc))\n              print(\"Group AUCs:\", group_aucs)\n\nend = time.time()\ntraining_time = (end - start)\nprint(f\"Training time per epoch: {training_time:.2f} sec\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T10:36:40.668018Z","iopub.execute_input":"2026-02-08T10:36:40.668250Z","iopub.status.idle":"2026-02-08T12:22:10.208514Z","shell.execute_reply.started":"2026-02-08T10:36:40.668225Z","shell.execute_reply":"2026-02-08T12:22:10.207718Z"}},"outputs":[{"name":"stdout","text":"Start Training\n------------------------------\nEpoch=0, BatchID=0, Val_AUC=0.4767, Best_Val_AUC=0.4767\nGroup AUCs: {0: 0.47246201704067214, 1: 0.4797506233163357}\nEpoch=0, BatchID=400, Val_AUC=0.6713, Best_Val_AUC=0.6713\nGroup AUCs: {0: 0.6825045390425128, 1: 0.6483459650141441}\nEpoch=0, BatchID=800, Val_AUC=0.6758, Best_Val_AUC=0.6758\nGroup AUCs: {0: 0.6796136120247287, 1: 0.6698635895269837}\nEpoch=0, BatchID=1200, Val_AUC=0.6948, Best_Val_AUC=0.6948\nGroup AUCs: {0: 0.687710952443678, 1: 0.6953653248016336}\nEpoch=0, BatchID=1600, Val_AUC=0.6773, Best_Val_AUC=0.6948\nGroup AUCs: {0: 0.6530487848955475, 1: 0.6946249128746447}\nEpoch=0, BatchID=2000, Val_AUC=0.6840, Best_Val_AUC=0.6948\nGroup AUCs: {0: 0.6633151817405532, 1: 0.6960110198071174}\nEpoch=0, BatchID=2400, Val_AUC=0.6876, Best_Val_AUC=0.6948\nGroup AUCs: {0: 0.6811634111249051, 1: 0.6882507826998794}\nEpoch=0, BatchID=2800, Val_AUC=0.6999, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.692915367103084, 1: 0.6982131634994435}\nEpoch=0, BatchID=3200, Val_AUC=0.6646, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6498243509471816, 1: 0.67215281771589}\nEpoch=0, BatchID=3600, Val_AUC=0.6731, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6612004698992712, 1: 0.6798005905554788}\nEpoch=0, BatchID=4000, Val_AUC=0.6907, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6797533335672827, 1: 0.6919878844984035}\nEpoch=0, BatchID=4400, Val_AUC=0.6858, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6835628994198902, 1: 0.6824998912681447}\nEpoch=0, BatchID=4800, Val_AUC=0.6812, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6560583503751921, 1: 0.6992328939635268}\nReducing learning rate to 0.01000 @ T=4848!\nUpdating regularizer @ T=4848!\nEpoch=1, BatchID=0, Val_AUC=0.6598, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6361479362186078, 1: 0.6759276719904969}\nEpoch=1, BatchID=400, Val_AUC=0.6880, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6605197052372969, 1: 0.707964611666864}\nEpoch=1, BatchID=800, Val_AUC=0.6989, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6826383243729012, 1: 0.707796898119685}\nEpoch=1, BatchID=1200, Val_AUC=0.6914, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6694226538645476, 1: 0.7050283337450237}\nEpoch=1, BatchID=1600, Val_AUC=0.6971, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6826418024209018, 1: 0.704692689636176}\nEpoch=1, BatchID=2000, Val_AUC=0.6883, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6763477700467103, 1: 0.6949131685372507}\nEpoch=1, BatchID=2400, Val_AUC=0.6923, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6757485325183756, 1: 0.7028810461006649}\nEpoch=1, BatchID=2800, Val_AUC=0.6883, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6731766719881683, 1: 0.6994701416036289}\nEpoch=1, BatchID=3200, Val_AUC=0.6917, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6694886526903929, 1: 0.7055985321906038}\nEpoch=1, BatchID=3600, Val_AUC=0.6947, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6821165066401795, 1: 0.7009996847000808}\nEpoch=1, BatchID=4000, Val_AUC=0.6899, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6712021543350742, 1: 0.7018514028034695}\nEpoch=1, BatchID=4400, Val_AUC=0.6872, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6716937549576509, 1: 0.6967476742878936}\nEpoch=1, BatchID=4800, Val_AUC=0.6914, Best_Val_AUC=0.6999\nGroup AUCs: {0: 0.6718306641908715, 1: 0.7045768688927257}\nTraining time per epoch: 6329.53 sec\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# show auc roc scores for each task \nauc_roc_score(test_true, test_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:22:10.209292Z","iopub.execute_input":"2026-02-08T12:22:10.209782Z","iopub.status.idle":"2026-02-08T12:22:10.222139Z","shell.execute_reply.started":"2026-02-08T12:22:10.209764Z","shell.execute_reply":"2026-02-08T12:22:10.221617Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[0.7568214032600993,\n 0.8982300884955753,\n 0.913218339440522,\n 0.0,\n 0.8888292158968851]"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df_test = pd.DataFrame({\n    \"age_group\": testSet.age_group,\n    \"Sex\": testSet.sex_str,\n    \"Age\": testSet.age\n})\n\ntest_true_df = pd.DataFrame(\n    test_true,\n    columns=labels_small\n)\n\ntest_pred_df = pd.DataFrame(\n    test_pred,\n    columns=labels_abbr_small\n)\n\ndf_test = pd.concat([df_test.reset_index(drop=True),\n                     test_true_df.reset_index(drop=True),\n                    test_pred_df.reset_index(drop=True)],\n                    axis=1)\n\n\ndf_test = df_test.sort_values(['age_group', 'Sex'])\n\nprint(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:22:10.222895Z","iopub.execute_input":"2026-02-08T12:22:10.223227Z","iopub.status.idle":"2026-02-08T12:22:10.244224Z","shell.execute_reply.started":"2026-02-08T12:22:10.223203Z","shell.execute_reply":"2026-02-08T12:22:10.243649Z"}},"outputs":[{"name":"stdout","text":"    age_group     Sex  Age  Cardiomegaly  Pneumonia  Pleural Effusion  \\\n17      15–19  Female   19           0.0        0.0               0.0   \n66      15–19  Female   18           0.0        0.0               0.0   \n67      15–19  Female   18           0.0        0.0               0.0   \n172     15–19    Male   19           0.0        0.0               0.0   \n68      20–24  Female   23           0.0        0.0               0.0   \n..        ...     ...  ...           ...        ...               ...   \n104     90–94    Male   90           0.0        0.0               0.0   \n126     90–94    Male   90           1.0        0.0               1.0   \n201     90–94    Male   90           0.0        0.0               0.0   \n210     90–94    Male   90           1.0        0.0               0.0   \n233     90–94    Male   90           0.0        0.0               1.0   \n\n     Fracture  No Finding        Cd        Pa        Ef        Fr        NF  \n17        0.0         1.0  0.004460  0.285352  0.006439  0.850935  0.959851  \n66        0.0         0.0  0.226482  0.031265  0.009807  0.580125  0.205909  \n67        0.0         0.0  0.908563  0.047452  0.043502  0.059980  0.154029  \n172       0.0         0.0  0.498112  0.324781  0.246044  0.030633  0.061175  \n68        0.0         1.0  0.000954  0.132215  0.004458  0.939797  0.928059  \n..        ...         ...       ...       ...       ...       ...       ...  \n104       0.0         0.0  0.022373  0.066958  0.005806  0.627473  0.980668  \n126       0.0         0.0  0.033957  0.213272  0.161356  0.981447  0.083166  \n201       0.0         0.0  0.067887  0.042575  0.066733  0.730405  0.769100  \n210       0.0         0.0  0.987264  0.397403  0.915985  0.046776  0.001487  \n233       0.0         0.0  0.793178  0.358160  0.923874  0.652128  0.040653  \n\n[234 rows x 13 columns]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"df_test.to_csv(\"/kaggle/working/\" + \"results_test_pred.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:22:10.244962Z","iopub.execute_input":"2026-02-08T12:22:10.245223Z","iopub.status.idle":"2026-02-08T12:22:10.269783Z","shell.execute_reply.started":"2026-02-08T12:22:10.245198Z","shell.execute_reply":"2026-02-08T12:22:10.269233Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# # training densenet121 with Original FIS approach for Sex attribute\n\n# from torch.utils.data import Subset\n\n# def endless_loader(dataloader):\n#     while True:\n#         for batch in dataloader:\n#             yield batch\n\n# sex_tensor = torch.tensor(traindSet.sex)  # shape (N,)\n\n# # print(sex_tensor.shape)            # (N,)\n# # print(sex_tensor.unique())  \n\n# female_indices = (sex_tensor == 0).nonzero(as_tuple=True)[0]\n# male_indices   = (sex_tensor == 1).nonzero(as_tuple=True)[0]\n\n# # print(len(female_indices), len(male_indices))\n# # print(len(female_indices) + len(male_indices) == len(sex_tensor))\n\n# female_dataset = Subset(traindSet, female_indices.tolist())\n# male_dataset   = Subset(traindSet, male_indices.tolist())\n\n# # x, y, attr = female_dataset[0]\n# # print(attr[\"Sex\"])  #\n\n# group_dataloaders = []\n\n# group_dataset_loader = torch.utils.data.DataLoader(female_dataset, batch_size=32, num_workers=0, shuffle=True, drop_last=True, pin_memory=False)\n# group_dataloaders.append(endless_loader(group_dataset_loader))\n\n# group_dataset_loader = torch.utils.data.DataLoader(male_dataset, batch_size=32, num_workers=0, shuffle=True, drop_last=True, pin_memory=False)\n# group_dataloaders.append(endless_loader(group_dataset_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:22:10.270512Z","iopub.execute_input":"2026-02-08T12:22:10.270932Z","iopub.status.idle":"2026-02-08T12:22:10.275641Z","shell.execute_reply.started":"2026-02-08T12:22:10.270914Z","shell.execute_reply":"2026-02-08T12:22:10.275037Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # training densenet121 with Original FIS approach for Sex attribute\n\n# from itertools import cycle\n\n# group_iters = [cycle(dl) for dl in group_dataloaders]\n\n# scaler = torch.amp.GradScaler('cuda')\n\n# criterion = nn.BCEWithLogitsLoss(reduction='none')\n\n# loss_scaler = Fair_Loss_Scaler(fair_scaling_coef=fair_scaling_coef, sinkhorn_blur=fair_scaling_sinkhorn_blur,\n#                                         fair_scaling_temperature=fair_scaling_temperature, right_asymptote=fair_right_asymptote)\n\n\n\n# print ('Start Training')\n# print ('-'*30)\n\n# # fis_loss = FISLoss(loss_fn, alpha=0.6)\n\n# # train_attributes_all_sex = torch.tensor(traindSet.sex)\n# # group_weights = compute_fis_group_weights(train_attributes_all_sex)\n\n# best_val_auc = 0 \n# for epoch in range(total_epochs):\n#     if epoch > 0:\n#         optimizer.update_regularizer(decay_factor=10)    \n\n#     for idx, data in enumerate(trainloader):\n        \n#         with torch.amp.autocast(device_type='cuda'):\n            \n#             # train_data, train_labels = data\n#             train_data, train_labels, train_attributes = data\n#             train_data, train_labels  = train_data.cuda(), train_labels.cuda()\n        \n#             y_pred = model(train_data)\n            \n#             smp_losses = []\n#             # for x in group_dataloaders:\n#             for x in group_iters:\n#                 smp_input, smp_target, smp_attr = next(x)\n#                 smp_input = smp_input.to(device)\n#                 smp_target = smp_target.to(device)\n#                 with torch.no_grad():\n#                     smp_pred = model(smp_input)\n#                     if smp_pred.shape[1] == 1:\n#                         smp_pred = smp_pred.squeeze(1)\n#                         smp_loss_raw = criterion(smp_pred, smp_target)\n#                         smp_loss = smp_loss_raw.mean(dim=1)\n#                     elif smp_pred.shape[1] > 1:\n#                         smp_loss_raw = criterion(smp_pred, smp_target.float())\n#                         # smp_loss = criterion(smp_pred, smp_target.long())\n#                         smp_loss = smp_loss_raw.mean(dim=1)\n#                         smp_losses.append(smp_loss)\n    \n#             if y_pred.shape[1] == 1:\n#                 y_pred = y_pred.squeeze(1)\n#                 loss = criterion(y_pred, train_labels)\n#                 pred_prob = torch.sigmoid(y_pred.detach())\n#                 loss_per_sample = loss.mean(dim=1)  # (B,)\n#             elif y_pred.shape[1] > 1:\n#                 # loss = criterion(y_pred, train_labels.long())\n#                 loss = criterion(y_pred, train_labels.float())\n#                 # pred_prob = F.softmax(y_pred.detach(), dim=1)\n#                 pred_prob = torch.sigmoid(y_pred.detach())\n#                 loss_per_sample = loss.mean(dim=1)  # (B,)\n                    \n            \n#             # print(\"main loss:\", loss.shape)              # (B,) or scalar\n#             # print(\"group loss:\", smp_losses[0].shape)    # (B,)\n#             # print(\"Sex attr:\", smp_attr[\"Sex\"].shape)\n#             # print(loss_per_sample.shape)        # torch.Size([32])\n#             # print(smp_losses[0].shape)          # torch.Size([32])\n#             # print(train_attributes[\"Sex\"].shape)#\n#             loss = loss_scaler(loss_per_sample, smp_losses, attr=train_attributes[\"Sex\"])\n\n    \n#         scaler.scale(loss).backward()\n#         scaler.step(optimizer)\n#         scaler.update()\n        \n#         optimizer.zero_grad()\n\n              \n#         # validation  \n#         if idx % 400 == 0:\n#             model.eval()\n#             with torch.no_grad():\n#                 test_pred = []\n#                 test_true = [] \n#                 test_attributes_true = [] \n#                 for jdx, data in enumerate(testloader):\n#                     # test_data, test_labels = data\n#                     test_data, test_labels, test_attributes = data\n#                     test_data = test_data.cuda()\n#                     y_pred = model(test_data)\n#                     y_pred = torch.sigmoid(y_pred)\n#                     test_pred.append(y_pred.cpu().detach().numpy())\n#                     test_true.append(test_labels.numpy())\n#                     test_attributes_true.append(test_attributes[\"Sex\"].cpu().numpy())\n                \n#                 test_true = np.concatenate(test_true)\n#                 test_pred = np.concatenate(test_pred)\n#                 test_attributes_true = np.concatenate(test_attributes_true)\n    \n#                 group_aucs = compute_group_auc(test_true, test_pred, test_attributes_true)\n                           \n#                 val_auc_mean = np.mean(auc_roc_score(test_true, test_pred)) \n#                 model.train()\n    \n#                 if best_val_auc < val_auc_mean:\n#                     best_val_auc = val_auc_mean\n#                     torch.save(model.state_dict(), 'aucm_pretrained_model_FIS_origin.pth')\n    \n#                 print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc))\n#                 print(\"Group AUCs:\", group_aucs)\n              ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:22:10.278325Z","iopub.execute_input":"2026-02-08T12:22:10.278558Z","iopub.status.idle":"2026-02-08T12:22:10.286964Z","shell.execute_reply.started":"2026-02-08T12:22:10.278543Z","shell.execute_reply":"2026-02-08T12:22:10.286333Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# training densenet121 with Original FIS approach for Age attribute\n\nscaler = torch.amp.GradScaler('cuda')\n\ncriterion = nn.BCEWithLogitsLoss(reduction='none')\n\nloss_scaler = Fair_Loss_Scaler(fair_scaling_coef=fair_scaling_coef, sinkhorn_blur=fair_scaling_sinkhorn_blur,\n                                        fair_scaling_temperature=fair_scaling_temperature, right_asymptote=fair_right_asymptote)\n\nprint ('Start Training')\nprint ('-'*30)\n\nbest_val_auc = 0 \n# Measure training time\nstart = time.time()\nfor epoch in range(total_epochs):\n    if epoch > 0:\n        optimizer.update_regularizer(decay_factor=10)    \n\n    for idx, data in enumerate(trainloader):\n        \n        with torch.amp.autocast(device_type='cuda'):\n            \n            # train_data, train_labels = data\n            train_data, train_labels, train_attributes = data\n            train_data, train_labels  = train_data.cuda(), train_labels.cuda() \n        \n            y_pred = model(train_data)\n            \n    \n            # Compute per-group loss inside the batch\n            smp_losses = []\n            smp_pred = model(train_data)\n            smp_loss_raw = criterion(smp_pred, train_labels.float())\n        \n            for g in range(17):  \n                mask = (train_attributes[\"AgeBin\"] == g)\n                \n                if mask.any():\n                    \n                    smp_loss = smp_loss_raw[mask].mean()\n                    smp_losses.append(smp_loss)\n                else:\n                    smp_loss = smp_loss_raw.new_tensor(0.0)\n                    smp_losses.append(smp_loss)\n                    \n            if y_pred.shape[1] == 1:\n                y_pred = y_pred.squeeze(1)\n                loss = criterion(y_pred, train_labels)\n                pred_prob = torch.sigmoid(y_pred.detach())\n                loss_per_sample = loss.mean(dim=1)  # (B,)\n            elif y_pred.shape[1] > 1:\n                # loss = criterion(y_pred, train_labels.long())\n                loss = criterion(y_pred, train_labels.float())\n                # pred_prob = F.softmax(y_pred.detach(), dim=1)\n                pred_prob = torch.sigmoid(y_pred.detach())\n                loss_per_sample = loss.mean(dim=1)  # (B,)\n                    \n            # print(loss_per_sample.shape)        # torch.Size([32])\n            # print(smp_losses[0].shape)          # torch.Size([32])\n            # print(train_attributes[\"AgeBin\"].shape)#\n            \n            loss = loss_scaler(loss_per_sample, smp_losses, attr=train_attributes[\"AgeBin\"])\n\n    \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        optimizer.zero_grad()\n\n              \n        # validation  \n        if idx % 400 == 0:\n            model.eval()\n            with torch.no_grad():\n                test_pred = []\n                test_true = [] \n                test_attributes_true = [] \n                for jdx, data in enumerate(testloader):\n                    # test_data, test_labels = data\n                    test_data, test_labels, test_attributes = data\n                    test_data = test_data.cuda()\n                    y_pred = model(test_data)\n                    y_pred = torch.sigmoid(y_pred)\n                    test_pred.append(y_pred.cpu().detach().numpy())\n                    test_true.append(test_labels.numpy())\n                    test_attributes_true.append(test_attributes[\"AgeBin\"].cpu().numpy())\n                \n                test_true = np.concatenate(test_true)\n                test_pred = np.concatenate(test_pred)\n                test_attributes_true = np.concatenate(test_attributes_true)\n    \n                group_aucs = compute_group_auc(test_true, test_pred, test_attributes_true)\n                group_aucs_mean = np.mean(list(group_aucs.values()))\n                           \n                val_auc_mean = np.mean(auc_roc_score(test_true, test_pred)) \n                model.train()\n    \n                if best_val_auc < val_auc_mean:\n                    best_val_auc = val_auc_mean\n                    torch.save(model.state_dict(), 'aucm_pretrained_model_FIS_origin_age.pth')\n    \n                print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc))\n                print(\"Mean Group AUCs:\", group_aucs_mean)\n\nend = time.time()\ntraining_time = (end - start)\nprint(f\"Training time per epoch: {training_time:.2f} sec\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:41:54.695966Z","iopub.execute_input":"2026-02-08T12:41:54.696579Z","iopub.status.idle":"2026-02-08T14:57:11.227083Z","shell.execute_reply.started":"2026-02-08T12:41:54.696556Z","shell.execute_reply":"2026-02-08T14:57:11.226354Z"}},"outputs":[{"name":"stdout","text":"Start Training\n------------------------------\nEpoch=0, BatchID=0, Val_AUC=0.6841, Best_Val_AUC=0.6841\nMean Group AUCs: 0.5347079594826736\nEpoch=0, BatchID=400, Val_AUC=0.6890, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5332128215288399\nEpoch=0, BatchID=800, Val_AUC=0.6855, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5291097606055273\nEpoch=0, BatchID=1200, Val_AUC=0.6834, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5258867444912069\nEpoch=0, BatchID=1600, Val_AUC=0.6808, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5244060471184041\nEpoch=0, BatchID=2000, Val_AUC=0.6800, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5231153566240524\nEpoch=0, BatchID=2400, Val_AUC=0.6795, Best_Val_AUC=0.6890\nMean Group AUCs: 0.523349594220933\nEpoch=0, BatchID=2800, Val_AUC=0.6769, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5203518866145296\nEpoch=0, BatchID=3200, Val_AUC=0.6803, Best_Val_AUC=0.6890\nMean Group AUCs: 0.522598417861061\nEpoch=0, BatchID=3600, Val_AUC=0.6831, Best_Val_AUC=0.6890\nMean Group AUCs: 0.523688312075669\nEpoch=0, BatchID=4000, Val_AUC=0.6771, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5210955397958259\nEpoch=0, BatchID=4400, Val_AUC=0.6800, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5222587611663699\nEpoch=0, BatchID=4800, Val_AUC=0.6789, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5230484931734932\nReducing learning rate to 0.00100 @ T=14544!\nUpdating regularizer @ T=14544!\nEpoch=1, BatchID=0, Val_AUC=0.6835, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5301796182332795\nEpoch=1, BatchID=400, Val_AUC=0.6805, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5231204074656592\nEpoch=1, BatchID=800, Val_AUC=0.6805, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5242031411570884\nEpoch=1, BatchID=1200, Val_AUC=0.6819, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5254494033744035\nEpoch=1, BatchID=1600, Val_AUC=0.6841, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5288019985354081\nEpoch=1, BatchID=2000, Val_AUC=0.6818, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5252527003190048\nEpoch=1, BatchID=2400, Val_AUC=0.6807, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5240271811521812\nEpoch=1, BatchID=2800, Val_AUC=0.6786, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5214693333935325\nEpoch=1, BatchID=3200, Val_AUC=0.6785, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5219593264922213\nEpoch=1, BatchID=3600, Val_AUC=0.6798, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5253772506733034\nEpoch=1, BatchID=4000, Val_AUC=0.6851, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5297134343218439\nEpoch=1, BatchID=4400, Val_AUC=0.6790, Best_Val_AUC=0.6890\nMean Group AUCs: 0.525835290192899\nEpoch=1, BatchID=4800, Val_AUC=0.6782, Best_Val_AUC=0.6890\nMean Group AUCs: 0.5211572732822733\nTraining time per epoch: 8116.52 sec\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# # show auc roc scores for each task \nauc_roc_score(test_true, test_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:57:11.228336Z","iopub.execute_input":"2026-02-08T14:57:11.228576Z","iopub.status.idle":"2026-02-08T14:57:11.239966Z","shell.execute_reply.started":"2026-02-08T14:57:11.228558Z","shell.execute_reply":"2026-02-08T14:57:11.239178Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[0.7226257973068745,\n 0.9275442477876106,\n 0.8907855929931182,\n 0.0,\n 0.8500268528464018]"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"df_test = pd.DataFrame({\n    \"age_group\": testSet.age_group,\n    \"Sex\": testSet.sex_str,\n    \"Age\": testSet.age\n})\n\ntest_true_df = pd.DataFrame(\n    test_true,\n    columns=labels_small\n)\n\ntest_pred_df = pd.DataFrame(\n    test_pred,\n    columns=labels_abbr_small\n)\n\ndf_test = pd.concat([df_test.reset_index(drop=True),\n                     test_true_df.reset_index(drop=True),\n                    test_pred_df.reset_index(drop=True)],\n                    axis=1)\n\n\ndf_test = df_test.sort_values(['age_group', 'Sex'])\n\nprint(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:57:11.240765Z","iopub.execute_input":"2026-02-08T14:57:11.240992Z","iopub.status.idle":"2026-02-08T14:57:11.266042Z","shell.execute_reply.started":"2026-02-08T14:57:11.240971Z","shell.execute_reply":"2026-02-08T14:57:11.265299Z"}},"outputs":[{"name":"stdout","text":"    age_group     Sex  Age  Cardiomegaly  Pneumonia  Pleural Effusion  \\\n17      15–19  Female   19           0.0        0.0               0.0   \n66      15–19  Female   18           0.0        0.0               0.0   \n67      15–19  Female   18           0.0        0.0               0.0   \n172     15–19    Male   19           0.0        0.0               0.0   \n68      20–24  Female   23           0.0        0.0               0.0   \n..        ...     ...  ...           ...        ...               ...   \n104     90–94    Male   90           0.0        0.0               0.0   \n126     90–94    Male   90           1.0        0.0               1.0   \n201     90–94    Male   90           0.0        0.0               0.0   \n210     90–94    Male   90           1.0        0.0               0.0   \n233     90–94    Male   90           0.0        0.0               1.0   \n\n     Fracture  No Finding        Cd        Pa        Ef        Fr        NF  \n17        0.0         1.0  0.051299  0.236260  0.044012  0.871538  0.224244  \n66        0.0         0.0  0.040679  0.014679  0.022963  0.869495  0.039130  \n67        0.0         0.0  0.839155  0.091996  0.108195  0.124207  0.062470  \n172       0.0         0.0  0.558236  0.081741  0.494439  0.029892  0.016176  \n68        0.0         1.0  0.020634  0.062423  0.060930  0.737482  0.125018  \n..        ...         ...       ...       ...       ...       ...       ...  \n104       0.0         0.0  0.269339  0.060778  0.063967  0.650666  0.593963  \n126       0.0         0.0  0.113304  0.039262  0.269981  0.870071  0.008005  \n201       0.0         0.0  0.164107  0.019809  0.263126  0.822950  0.097226  \n210       0.0         0.0  0.899255  0.242637  0.767937  0.119882  0.003637  \n233       0.0         0.0  0.568192  0.211911  0.834387  0.673125  0.030376  \n\n[234 rows x 13 columns]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"df_test.to_csv(\"/kaggle/working/\" + \"FIS_origin_results_test_pred_age.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T14:57:11.267793Z","iopub.execute_input":"2026-02-08T14:57:11.268167Z","iopub.status.idle":"2026-02-08T14:57:11.286718Z","shell.execute_reply.started":"2026-02-08T14:57:11.268150Z","shell.execute_reply":"2026-02-08T14:57:11.286065Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# FIS code for sex groups\n\nclass Fair_Loss_Scaler_sex(nn.Module):\n    def __init__(self, level='individual', fair_scaling_group_weights=None, \n                    fair_scaling_temperature=1., fair_scaling_coef=.5, sinkhorn_blur=0.1, right_asymptote=2.):\n        super().__init__()\n        self.level = level\n        self.fair_scaling_temperature = fair_scaling_temperature\n        self.sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=sinkhorn_blur)\n        self.fair_scaling_coef = fair_scaling_coef\n        #self.fair_scaling_coef = nn.Parameter(torch.tensor(fair_scaling_coef))\n        self.right_asymptote = right_asymptote\n        \n    def forward(self, x, smp_xs, attr):\n                \n        # x: (B, C) → reduce to (B,)\n        if x.ndim == 2:\n            x = x.mean(dim=1)\n\n        # individual weights\n        individual_weights = torch.softmax(x.detach() / self.fair_scaling_temperature, dim=0) * self.right_asymptote\n    \n        # total distribution\n        ttl_smp_x = torch.cat(smp_xs)\n        # ttl_smp_x = torch.stack(smp_xs)  # [num_groups]\n    \n        distances_distributions = torch.zeros(len(smp_xs), device=x.device)\n        # distances_distributions = torch.zeros(len(smp_xs), device=x.device, dtype=x.dtype)\n    \n        for i, smp_x in enumerate(smp_xs):\n            distances_distributions[i] = self.sinkhorn_loss(\n                smp_x.view(1, -1, 1),\n                ttl_smp_x.view(1, -1, 1)\n            )\n\n        # for i, smp_x in enumerate(smp_xs):\n        #     distances_distributions[i] = self.sinkhorn_loss(\n        #         smp_x.view(1, 1, 1),           # (B=1, N=1, D=1)\n        #         ttl_smp_x.view(1, -1, 1)       # (B=1, M=G, D=1)\n        #     )\n    \n        group_weights = torch.softmax(\n            distances_distributions[attr.long()] / self.fair_scaling_temperature,\n            dim=0\n        ) * self.right_asymptote\n    \n        loss = (\n            ((1 - self.fair_scaling_coef) * individual_weights\n             + self.fair_scaling_coef * group_weights)\n            * x\n        ).mean()\n    \n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:20:46.047492Z","iopub.execute_input":"2026-02-08T15:20:46.048196Z","iopub.status.idle":"2026-02-08T15:20:46.055360Z","shell.execute_reply.started":"2026-02-08T15:20:46.048158Z","shell.execute_reply":"2026-02-08T15:20:46.054574Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# training densenet121 with Original FIS approach for Sex attribute\n\nscaler = torch.amp.GradScaler('cuda')\n\ncriterion = nn.BCEWithLogitsLoss(reduction='none')\n\nloss_scaler = Fair_Loss_Scaler_sex(fair_scaling_coef=fair_scaling_coef, sinkhorn_blur=fair_scaling_sinkhorn_blur,\n                                        fair_scaling_temperature=fair_scaling_temperature, right_asymptote=fair_right_asymptote)\n\n\n\nprint ('Start Training')\nprint ('-'*30)\n\nbest_val_auc = 0 \n# Measure training time\nstart = time.time()\nfor epoch in range(total_epochs):\n    if epoch > 0:\n        optimizer.update_regularizer(decay_factor=10)    \n\n    for idx, data in enumerate(trainloader):\n        \n        with torch.amp.autocast(device_type='cuda'):\n            \n            # train_data, train_labels = data\n            train_data, train_labels, train_attributes = data\n            train_data, train_labels  = train_data.cuda(), train_labels.cuda()\n        \n            y_pred = model(train_data)\n            \n   \n            # Compute per-group loss inside the batch\n            smp_losses = []\n            smp_pred = model(train_data)\n            smp_loss_raw = criterion(smp_pred, train_labels.float())\n        \n            for g in [0, 1]:  # example for Sex\n                mask = (train_attributes[\"Sex\"] == g)\n                \n                if mask.any():\n                    # smp_loss_raw = criterion(smp_pred, smp_target.float())\n                    smp_loss = smp_loss_raw[mask].mean(dim=1)\n                    smp_losses.append(smp_loss)\n                    \n            if y_pred.shape[1] == 1:\n                y_pred = y_pred.squeeze(1)\n                loss = criterion(y_pred, train_labels)\n                pred_prob = torch.sigmoid(y_pred.detach())\n                loss_per_sample = loss.mean(dim=1)  # (B,)\n            elif y_pred.shape[1] > 1:\n                # loss = criterion(y_pred, train_labels.long())\n                loss = criterion(y_pred, train_labels.float())\n                # pred_prob = F.softmax(y_pred.detach(), dim=1)\n                pred_prob = torch.sigmoid(y_pred.detach())\n                loss_per_sample = loss.mean(dim=1)  # (B,)\n                    \n            \n            loss = loss_scaler(loss_per_sample, smp_losses, attr=train_attributes[\"Sex\"])\n\n    \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        optimizer.zero_grad()\n\n              \n        # validation  \n        if idx % 400 == 0:\n            model.eval()\n            with torch.no_grad():\n                test_pred = []\n                test_true = [] \n                test_attributes_true = [] \n                for jdx, data in enumerate(testloader):\n                    # test_data, test_labels = data\n                    test_data, test_labels, test_attributes = data\n                    test_data = test_data.cuda()\n                    y_pred = model(test_data)\n                    y_pred = torch.sigmoid(y_pred)\n                    test_pred.append(y_pred.cpu().detach().numpy())\n                    test_true.append(test_labels.numpy())\n                    test_attributes_true.append(test_attributes[\"Sex\"].cpu().numpy())\n                \n                test_true = np.concatenate(test_true)\n                test_pred = np.concatenate(test_pred)\n                test_attributes_true = np.concatenate(test_attributes_true)\n    \n                group_aucs = compute_group_auc(test_true, test_pred, test_attributes_true)\n                           \n                val_auc_mean = np.mean(auc_roc_score(test_true, test_pred)) \n                model.train()\n    \n                if best_val_auc < val_auc_mean:\n                    best_val_auc = val_auc_mean\n                    torch.save(model.state_dict(), 'aucm_pretrained_model_FIS_origin.pth')\n    \n                print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc))\n                print(\"Group AUCs:\", group_aucs)\n\nend = time.time()\ntraining_time = (end - start)\nprint(f\"Training time per epoch: {training_time:.2f} sec\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:20:46.056501Z","iopub.execute_input":"2026-02-08T15:20:46.056811Z","iopub.status.idle":"2026-02-08T17:16:28.897097Z","shell.execute_reply.started":"2026-02-08T15:20:46.056793Z","shell.execute_reply":"2026-02-08T17:16:28.896159Z"}},"outputs":[{"name":"stdout","text":"Start Training\n------------------------------\nEpoch=0, BatchID=0, Val_AUC=0.6787, Best_Val_AUC=0.6787\nGroup AUCs: {0: 0.6633381160731556, 1: 0.6919479209166528}\nEpoch=0, BatchID=400, Val_AUC=0.6822, Best_Val_AUC=0.6822\nGroup AUCs: {0: 0.6646649213701095, 1: 0.6967254555407348}\nEpoch=0, BatchID=800, Val_AUC=0.6836, Best_Val_AUC=0.6836\nGroup AUCs: {0: 0.6646373465321491, 1: 0.7001289827498474}\nEpoch=0, BatchID=1200, Val_AUC=0.6794, Best_Val_AUC=0.6836\nGroup AUCs: {0: 0.6626063355197431, 1: 0.6931282164567372}\nEpoch=0, BatchID=1600, Val_AUC=0.6820, Best_Val_AUC=0.6836\nGroup AUCs: {0: 0.6651267224313921, 1: 0.6965455251885827}\nEpoch=0, BatchID=2000, Val_AUC=0.6812, Best_Val_AUC=0.6836\nGroup AUCs: {0: 0.6640515195607989, 1: 0.6949609952596505}\nEpoch=0, BatchID=2400, Val_AUC=0.6808, Best_Val_AUC=0.6836\nGroup AUCs: {0: 0.6643219469473924, 1: 0.6945355606909935}\nEpoch=0, BatchID=2800, Val_AUC=0.6835, Best_Val_AUC=0.6836\nGroup AUCs: {0: 0.6639420486950455, 1: 0.6999904113001217}\nEpoch=0, BatchID=3200, Val_AUC=0.6850, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6652244037722818, 1: 0.7014180475938995}\nEpoch=0, BatchID=3600, Val_AUC=0.6827, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6645507799924425, 1: 0.6979102056768915}\nEpoch=0, BatchID=4000, Val_AUC=0.6836, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6660527703425375, 1: 0.6982126843237725}\nEpoch=0, BatchID=4400, Val_AUC=0.6794, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6623832467805254, 1: 0.693446393329035}\nEpoch=0, BatchID=4800, Val_AUC=0.6776, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6609115995002977, 1: 0.6911392420057134}\nReducing learning rate to 0.00010 @ T=24240!\nUpdating regularizer @ T=24240!\nEpoch=1, BatchID=0, Val_AUC=0.6831, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6642894332338031, 1: 0.6984869669322272}\nEpoch=1, BatchID=400, Val_AUC=0.6809, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6637141707739199, 1: 0.6948656108164399}\nEpoch=1, BatchID=800, Val_AUC=0.6813, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6635706577250521, 1: 0.6959565058203785}\nEpoch=1, BatchID=1200, Val_AUC=0.6823, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.6644444281131928, 1: 0.6971358873390472}\nEpoch=1, BatchID=1600, Val_AUC=0.6809, Best_Val_AUC=0.6850\nGroup AUCs: {0: 0.663352664336327, 1: 0.6954881889394305}\nEpoch=1, BatchID=2000, Val_AUC=0.6874, Best_Val_AUC=0.6874\nGroup AUCs: {0: 0.668475242302937, 1: 0.7033404075605627}\nEpoch=1, BatchID=2400, Val_AUC=0.6823, Best_Val_AUC=0.6874\nGroup AUCs: {0: 0.6636170397227461, 1: 0.6974653044715333}\nEpoch=1, BatchID=2800, Val_AUC=0.6825, Best_Val_AUC=0.6874\nGroup AUCs: {0: 0.6636533807131761, 1: 0.697663281881787}\nEpoch=1, BatchID=3200, Val_AUC=0.6832, Best_Val_AUC=0.6874\nGroup AUCs: {0: 0.664794587665, 1: 0.6980780501106795}\nEpoch=1, BatchID=3600, Val_AUC=0.6880, Best_Val_AUC=0.6880\nGroup AUCs: {0: 0.6684890890569143, 1: 0.7032386602516955}\nEpoch=1, BatchID=4000, Val_AUC=0.6846, Best_Val_AUC=0.6880\nGroup AUCs: {0: 0.6659236706121049, 1: 0.6978993646106508}\nEpoch=1, BatchID=4400, Val_AUC=0.6821, Best_Val_AUC=0.6880\nGroup AUCs: {0: 0.6643082188634093, 1: 0.6972726129813004}\nEpoch=1, BatchID=4800, Val_AUC=0.6832, Best_Val_AUC=0.6880\nGroup AUCs: {0: 0.6646118236679166, 1: 0.6987102885230102}\nTraining time per epoch: 6942.82 sec\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# # show auc roc scores for each task \nauc_roc_score(test_true, test_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T17:16:28.897903Z","iopub.execute_input":"2026-02-08T17:16:28.898138Z","iopub.status.idle":"2026-02-08T17:16:28.909351Z","shell.execute_reply.started":"2026-02-08T17:16:28.898120Z","shell.execute_reply":"2026-02-08T17:16:28.908715Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[0.7346739900779589,\n 0.9231194690265487,\n 0.8974886048797927,\n 0.0,\n 0.860499462943072]"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"df_test = pd.DataFrame({\n    \"age_group\": testSet.age_group,\n    \"Sex\": testSet.sex_str,\n    \"Age\": testSet.age\n})\n\ntest_true_df = pd.DataFrame(\n    test_true,\n    columns=labels_small\n)\n\ntest_pred_df = pd.DataFrame(\n    test_pred,\n    columns=labels_abbr_small\n)\n\ndf_test = pd.concat([df_test.reset_index(drop=True),\n                     test_true_df.reset_index(drop=True),\n                    test_pred_df.reset_index(drop=True)],\n                    axis=1)\n\n\ndf_test = df_test.sort_values(['age_group', 'Sex'])\n\nprint(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T17:16:28.911093Z","iopub.execute_input":"2026-02-08T17:16:28.911374Z","iopub.status.idle":"2026-02-08T17:16:28.927401Z","shell.execute_reply.started":"2026-02-08T17:16:28.911358Z","shell.execute_reply":"2026-02-08T17:16:28.926546Z"}},"outputs":[{"name":"stdout","text":"    age_group     Sex  Age  Cardiomegaly  Pneumonia  Pleural Effusion  \\\n17      15–19  Female   19           0.0        0.0               0.0   \n66      15–19  Female   18           0.0        0.0               0.0   \n67      15–19  Female   18           0.0        0.0               0.0   \n172     15–19    Male   19           0.0        0.0               0.0   \n68      20–24  Female   23           0.0        0.0               0.0   \n..        ...     ...  ...           ...        ...               ...   \n104     90–94    Male   90           0.0        0.0               0.0   \n126     90–94    Male   90           1.0        0.0               1.0   \n201     90–94    Male   90           0.0        0.0               0.0   \n210     90–94    Male   90           1.0        0.0               0.0   \n233     90–94    Male   90           0.0        0.0               1.0   \n\n     Fracture  No Finding        Cd        Pa        Ef        Fr        NF  \n17        0.0         1.0  0.076875  0.253473  0.039786  0.811251  0.235118  \n66        0.0         0.0  0.082316  0.017043  0.024342  0.769619  0.047941  \n67        0.0         0.0  0.876794  0.086879  0.104045  0.084592  0.055738  \n172       0.0         0.0  0.654707  0.097184  0.429076  0.018785  0.013397  \n68        0.0         1.0  0.034652  0.075995  0.059885  0.632892  0.144500  \n..        ...         ...       ...       ...       ...       ...       ...  \n104       0.0         0.0  0.333072  0.050421  0.059676  0.537470  0.588215  \n126       0.0         0.0  0.186114  0.047792  0.272861  0.779413  0.009510  \n201       0.0         0.0  0.255878  0.021055  0.254407  0.717129  0.113404  \n210       0.0         0.0  0.926881  0.253900  0.713765  0.077038  0.003163  \n233       0.0         0.0  0.658586  0.207923  0.809464  0.557264  0.029197  \n\n[234 rows x 13 columns]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"df_test.to_csv(\"/kaggle/working/\" + \"FIS_origin_results_test_pred.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T17:18:13.935281Z","iopub.execute_input":"2026-02-08T17:18:13.935862Z","iopub.status.idle":"2026-02-08T17:18:13.943517Z","shell.execute_reply.started":"2026-02-08T17:18:13.935837Z","shell.execute_reply":"2026-02-08T17:18:13.942720Z"}},"outputs":[],"execution_count":26}]}